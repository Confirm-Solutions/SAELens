{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYWV3AuAxXjO"
      },
      "source": [
        "# Setup & loading stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SCvWyLnxxEpP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_dataset\n",
        "from IPython import get_ipython\n",
        "from sae_lens import SAE, HookedSAETransformer\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from sae_vis.data_config_classes import SaeVisConfig, SaeVisLayoutConfig\n",
        "from sae_vis.data_storing_fns import SaeVisData\n",
        "from sae_vis.model_fns import (\n",
        "    load_demo_model_saes_and_data,\n",
        "    load_othello_vocab,\n",
        ")\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8c75RL6_x3Go"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame, display\n",
        "import os\n",
        "\n",
        "def display_vis_inline(filename: str, height: int = 850):\n",
        "    \"\"\"\n",
        "    Displays the HTML file inline in a Jupyter notebook.\n",
        "    \"\"\"\n",
        "    # If the file is not in the current directory, adjust the path as needed\n",
        "    display(IFrame(src=filename, width='100%', height=height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fYvNvSe50GKG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gelu-1l into HookedTransformer\n",
            "torch.Size([215402, 128])\n"
          ]
        }
      ],
      "source": [
        "# Setup for basic model (examples 1-3)\n",
        "\n",
        "SEQ_LEN = 128\n",
        "DATASET_PATH = \"NeelNanda/c4-code-20k\"\n",
        "MODEL_NAME = \"gelu-1l\"\n",
        "HOOK_NAME = \"blocks.0.mlp.hook_post\"\n",
        "\n",
        "# For this, it's just a 1L model from Neel's library\n",
        "sae, sae_B, model, all_tokens = load_demo_model_saes_and_data(SEQ_LEN, str(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h8WBwmotyhVY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model othello-gpt into HookedTransformer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/SAELens/sae_lens/sae.py:159: UserWarning: \n",
            "This SAE has non-empty model_from_pretrained_kwargs. \n",
            "For optimal performance, load the model like so:\n",
            "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "othello_tokens.shape=torch.Size([5000, 59])\n",
            "Alive features: 6784/8192\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Othello setup (example 4)\n",
        "\n",
        "hf_repo_id = \"callummcdougall/arena-demos-othellogpt\"\n",
        "sae_id = \"blocks.5.mlp.hook_post-v1\"\n",
        "model_name = \"othello-gpt\"\n",
        "\n",
        "othellogpt: HookedSAETransformer = HookedSAETransformer.from_pretrained(model_name)\n",
        "othellogpt_sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
        "\n",
        "def hf_othello_load(filename: str):\n",
        "    path = hf_hub_download(repo_id=hf_repo_id, filename=filename)\n",
        "    return torch.load(path, weights_only=True, map_location=device)\n",
        "\n",
        "othello_tokens = hf_othello_load(\"tokens.pt\")[:5000]\n",
        "othello_target_logits = hf_othello_load(\"target_logits.pt\")[:5000]\n",
        "othello_linear_probes = hf_othello_load(\"linear_probes.pt\")\n",
        "print(f\"{othello_tokens.shape=}\")\n",
        "\n",
        "# Get live features\n",
        "_, cache = othellogpt.run_with_cache_with_saes(\n",
        "    othello_tokens[:1000],\n",
        "    saes=[othellogpt_sae],\n",
        "    names_filter=(post_acts_hook := f\"{othellogpt_sae.cfg.hook_name}.hook_sae_acts_post\"),\n",
        ")\n",
        "acts = cache[post_acts_hook]\n",
        "othello_alive_feats = (acts[:, 5:-5].flatten(0, 1) > 1e-8).any(dim=0).nonzero().squeeze().tolist()\n",
        "print(f\"Alive features: {len(othello_alive_feats)}/{othellogpt_sae.cfg.d_sae}\")\n",
        "\n",
        "del cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qiH7-j2NzVA1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_ln=False instead.\n",
            "WARNING:root:You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_ln=False instead.\n",
            "WARNING:root:You tried to specify center_unembed=True for a shortformer model, but this can't be done! Setting center_unembed=False instead.\n",
            "WARNING:root:You tried to specify center_writing_weights=True for a shortformer model, but this can't be done! Setting center_writing_weights=False instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model attn-only-2l-demo into HookedTransformer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/SAELens/sae_lens/sae.py:159: UserWarning: \n",
            "This SAE has non-empty model_from_pretrained_kwargs. \n",
            "For optimal performance, load the model like so:\n",
            "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f90c45cd1ce34512800a79a2432c4202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2226f8ca3de444afa4680e99e4e4e041",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 22.07 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 20.69 GiB memory in use. Of the allocated memory 19.87 GiB is allocated by PyTorch, and 568.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m tokens.shape == (batch_size, seq_len)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get live features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m _, cache = \u001b[43mattn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache_with_saes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msaes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattn_sae\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost_acts_hook\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mattn_sae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.hook_sae_acts_post\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_at_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_sae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhook_layer\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m acts = cache[post_acts_hook]\n\u001b[32m     26\u001b[39m attn_alive_feats = (acts.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1e-8\u001b[39m).any(dim=\u001b[32m0\u001b[39m).nonzero().squeeze().tolist()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SAELens/sae_lens/analysis/hooked_sae_transformer.py:218\u001b[39m, in \u001b[36mHookedSAETransformer.run_with_cache_with_saes\u001b[39m\u001b[34m(self, saes, reset_saes_end, use_error_term, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\u001b[39;00m\n\u001b[32m    200\u001b[39m \n\u001b[32m    201\u001b[39m \u001b[33;03mAttaches given SAEs before running the model with cache and then removes them.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m \u001b[33;03m    **kwargs: Keyword arguments for the model forward pass\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.saes(\n\u001b[32m    216\u001b[39m     saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term\n\u001b[32m    217\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_cache_object\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_cache_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_lens/HookedTransformer.py:694\u001b[39m, in \u001b[36mHookedTransformer.run_with_cache\u001b[39m\u001b[34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_with_cache\u001b[39m(\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m, *model_args, return_cache_object=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs\n\u001b[32m    679\u001b[39m ) -> Tuple[\n\u001b[32m   (...)\u001b[39m\u001b[32m    686\u001b[39m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]],\n\u001b[32m    687\u001b[39m ]:\n\u001b[32m    688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[32m    689\u001b[39m \n\u001b[32m    690\u001b[39m \u001b[33;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[33;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     out, cache_dict = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[32m    698\u001b[39m         cache = ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim=\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_lens/hook_points.py:569\u001b[39m, in \u001b[36mHookedRootModule.run_with_cache\u001b[39m\u001b[34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m cache_dict, fwd, bwd = \u001b[38;5;28mself\u001b[39m.get_caching_hooks(\n\u001b[32m    556\u001b[39m     names_filter,\n\u001b[32m    557\u001b[39m     incl_bwd,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     pos_slice=pos_slice,\n\u001b[32m    561\u001b[39m )\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(\n\u001b[32m    564\u001b[39m     fwd_hooks=fwd,\n\u001b[32m    565\u001b[39m     bwd_hooks=bwd,\n\u001b[32m    566\u001b[39m     reset_hooks_end=reset_hooks_end,\n\u001b[32m    567\u001b[39m     clear_contexts=clear_contexts,\n\u001b[32m    568\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[32m    571\u001b[39m         model_out.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_lens/HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_lens/components/transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_lens/components/abstract_attention.py:264\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    262\u001b[39m pattern = pattern.to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)\n\u001b[32m    263\u001b[39m pattern = pattern.to(v.device)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_z_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_attn_result:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.load_in_4bit:\n\u001b[32m    267\u001b[39m         \u001b[38;5;66;03m# call bitsandbytes method to dequantize and multiply\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformer_lens/components/abstract_attention.py:438\u001b[39m, in \u001b[36mAbstractAttention.calculate_z_scores\u001b[39m\u001b[34m(self, v, pattern)\u001b[39m\n\u001b[32m    431\u001b[39m v_ = einops.rearrange(\n\u001b[32m    432\u001b[39m     v, \u001b[33m\"\u001b[39m\u001b[33mbatch key_pos head_index d_head -> batch head_index key_pos d_head\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    433\u001b[39m )\n\u001b[32m    434\u001b[39m pattern_ = einops.rearrange(\n\u001b[32m    435\u001b[39m     pattern,\n\u001b[32m    436\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch head_index query_pos key_pos -> batch head_index query_pos key_pos\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    437\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhook_z\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43meinops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpattern_\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch head_index query_pos d_head -> batch query_pos head_index d_head\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SAELens/sae_lens/sae.py:408\u001b[39m, in \u001b[36mSAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    406\u001b[39m     sae_out, feature_acts = \u001b[38;5;28mself\u001b[39m.encode_ridge(x)\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     feature_acts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m     sae_out = \u001b[38;5;28mself\u001b[39m.decode(feature_acts)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# TEMP\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/SAELens/sae_lens/sae.py:431\u001b[39m, in \u001b[36mSAE.encode_gated\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    428\u001b[39m sae_in = \u001b[38;5;28mself\u001b[39m.process_sae_in(x)\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Gating path\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m gating_pre_activation = \u001b[43msae_in\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_enc\u001b[49m + \u001b[38;5;28mself\u001b[39m.b_gate\n\u001b[32m    432\u001b[39m active_features = (gating_pre_activation > \u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Magnitude path with weight sharing\u001b[39;00m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 22.07 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 20.69 GiB memory in use. Of the allocated memory 19.87 GiB is allocated by PyTorch, and 568.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Attention model setup (example 5)\n",
        "\n",
        "attn_model: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"attn-only-2l-demo\")\n",
        "hf_repo_id = \"callummcdougall/arena-demos-attn2l\"\n",
        "sae_id = \"blocks.0.attn.hook_z-v2\"\n",
        "attn_sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]\n",
        "\n",
        "original_dataset = load_dataset(attn_sae.cfg.dataset_path, split=\"train\", streaming=True, trust_remote_code=True)\n",
        "batch_size = 4096\n",
        "seq_len = 256\n",
        "seq_list = [x[\"input_ids\"][: seq_len - 1] for (_, x) in zip(range(batch_size), original_dataset)]\n",
        "tokens = torch.tensor(seq_list, device=device)\n",
        "assert attn_model.tokenizer is not None\n",
        "bos_token = torch.tensor([attn_model.tokenizer.bos_token_id for _ in range(batch_size)], device=device)\n",
        "tokens = torch.cat([bos_token.unsqueeze(1), tokens], dim=1)\n",
        "assert tokens.shape == (batch_size, seq_len)\n",
        "\n",
        "# Get live features\n",
        "_, cache = attn_model.run_with_cache_with_saes(\n",
        "    tokens[:512],\n",
        "    saes=[attn_sae],\n",
        "    names_filter=(post_acts_hook := f\"{attn_sae.cfg.hook_name}.hook_sae_acts_post\"),\n",
        "    stop_at_layer=attn_sae.cfg.hook_layer + 1,\n",
        ")\n",
        "acts = cache[post_acts_hook]\n",
        "attn_alive_feats = (acts.flatten(0, 1) > 1e-8).any(dim=0).nonzero().squeeze().tolist()\n",
        "print(f\"Alive features: {len(attn_alive_feats)}/{attn_sae.cfg.d_sae}\")\n",
        "\n",
        "del cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pdf9hLTxhIz"
      },
      "source": [
        "# Demos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mBMwdnKGxe8i"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77a0707cb1b64cf19276a273fd1f1769",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Forward passes to cache data for vis:   0%|          | 0/128 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d840c66a5b554dbf908011ef168969d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting vis data from cached data:   0%|          | 0/128 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time   </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│ (1) Forward passes to gather model activations │ 21.01s │ 43.7% │\n",
              "│ (2) Getting data for sequences                 │ 15.89s │ 33.1% │\n",
              "│ (3) Getting data for non-sequence components   │ 3.45s  │ 7.2%  │\n",
              "│ (?) Unaccounted time                           │ 7.72s  │ 16.1% │\n",
              "└────────────────────────────────────────────────┴────────┴───────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│ (1) Forward passes to gather model activations │ 21.01s │ 43.7% │\n",
              "│ (2) Getting data for sequences                 │ 15.89s │ 33.1% │\n",
              "│ (3) Getting data for non-sequence components   │ 3.45s  │ 7.2%  │\n",
              "│ (?) Unaccounted time                           │ 7.72s  │ 16.1% │\n",
              "└────────────────────────────────────────────────┴────────┴───────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"850\"\n",
              "            src=\"demo_feature_vis.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f5b800d56d0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# [1/5] Basic demo, 1L model, default settings\n",
        "\n",
        "sae_vis_data = SaeVisData.create(\n",
        "    sae=sae,\n",
        "    sae_B=sae_B,\n",
        "    model=model,\n",
        "    tokens=all_tokens[:8192],\n",
        "    cfg=SaeVisConfig(features=range(128)),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "filename = \"demo_feature_vis.html\"\n",
        "sae_vis_data.save_feature_centric_vis(filename, feature=8)\n",
        "display_vis_inline(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Eywgnw0Mxtn9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SaeVisLayoutConfig\n",
              "\n",
              "Key: \n",
              "  the tree shows which components will be displayed in each column (from left to right)\n",
              "  arguments are <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">green</span>\n",
              "  arguments changed from their default are <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">orange</span>, with default in brackets\n",
              "  argument descriptions are in <span style=\"font-style: italic\">italics</span>\n",
              "\n",
              "├── Column 0\n",
              "│   └── SeqMultiGroup\n",
              "│       ├── <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">buffer: None</span> (default = (5, 5)) \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">How many tokens to add as context to each sequence, on each side. The tokens chosen for the top acts / </span>\n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">quantile groups can't be outside the buffer range. If None, we use the entire sequence as context.</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">compute_buffer: True</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">If False, then we don't compute the loss effect, activations, or any other data for tokens other than </span>\n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">the bold tokens in our sequences (saving time).</span>\n",
              "│       ├── <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">n_quantiles: 0</span> (default = 10) \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Number of quantile groups for the sequences. If zero, we only show top activations, no quantile groups.</span>\n",
              "│       ├── <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">top_acts_group_size: 30</span> (default = 20) \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Number of sequences in the 'top activating sequences' group.</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">quantile_group_size: 5</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Number of sequences in each of the sequence quantile groups.</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">top_logits_hoverdata: 5</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Number of top/bottom logits to show in the hoverdata for each token.</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">hover_below: True</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Whether the hover information about a token appears below or above the token.</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">othello: False</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">If True, we make Othello boards instead of sequences (requires OthelloGPT)</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">n_boards_per_row: 3</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Only relevant for Othello, sets number of boards per row in top examples</span>\n",
              "│       ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">dfa_for_attn_saes: True</span> \n",
              "│       │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Only relevant for attention SAEs. If true, shows DFA for top attention tokens.</span>\n",
              "│       └── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">dfa_buffer: (5, 5)</span> \n",
              "│           \n",
              "│           \n",
              "└── Column 1\n",
              "    ├── ActsHistogram\n",
              "    │   └── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">n_bins: 50</span> \n",
              "    │       <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Number of bins for the histogram.</span>\n",
              "    │       \n",
              "    └── FeatureTables\n",
              "        ├── <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">n_rows: 5</span> (default = 3) \n",
              "        │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Number of rows to show for each feature table.</span>\n",
              "        ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">neuron_alignment_table: True</span> \n",
              "        │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Whether to show the neuron alignment table.</span>\n",
              "        ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">correlated_neurons_table: True</span> \n",
              "        │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Whether to show the correlated neurons table.</span>\n",
              "        ├── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">correlated_features_table: True</span> \n",
              "        │   <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Whether to show the (pairwise) correlated features table.</span>\n",
              "        └── <span style=\"color: #00aa00; text-decoration-color: #00aa00; font-weight: bold\">correlated_b_features_table: False</span> \n",
              "            <span style=\"color: #888888; text-decoration-color: #888888; font-style: italic\">Whether to show the correlated encoder-B features table.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "SaeVisLayoutConfig\n",
              "\n",
              "Key: \n",
              "  the tree shows which components will be displayed in each column (from left to right)\n",
              "  arguments are \u001b[1;38;2;0;170;0mgreen\u001b[0m\n",
              "  arguments changed from their default are \u001b[1;38;5;208morange\u001b[0m, with default in brackets\n",
              "  argument descriptions are in \u001b[3mitalics\u001b[0m\n",
              "\n",
              "├── Column 0\n",
              "│   └── SeqMultiGroup\n",
              "│       ├── \u001b[1;38;5;208mbuffer: None\u001b[0m (default = (5, 5)) \n",
              "│       │   \u001b[3;38;2;136;136;136mHow many tokens to add as context to each sequence, on each side. The tokens chosen for the top acts / \u001b[0m\n",
              "│       │   \u001b[3;38;2;136;136;136mquantile groups can't be outside the buffer range. If None, we use the entire sequence as context.\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mcompute_buffer: True\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mIf False, then we don't compute the loss effect, activations, or any other data for tokens other than \u001b[0m\n",
              "│       │   \u001b[3;38;2;136;136;136mthe bold tokens in our sequences (saving time).\u001b[0m\n",
              "│       ├── \u001b[1;38;5;208mn_quantiles: 0\u001b[0m (default = 10) \n",
              "│       │   \u001b[3;38;2;136;136;136mNumber of quantile groups for the sequences. If zero, we only show top activations, no quantile groups.\u001b[0m\n",
              "│       ├── \u001b[1;38;5;208mtop_acts_group_size: 30\u001b[0m (default = 20) \n",
              "│       │   \u001b[3;38;2;136;136;136mNumber of sequences in the 'top activating sequences' group.\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mquantile_group_size: 5\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mNumber of sequences in each of the sequence quantile groups.\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mtop_logits_hoverdata: 5\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mNumber of top/bottom logits to show in the hoverdata for each token.\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mhover_below: True\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mWhether the hover information about a token appears below or above the token.\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mothello: False\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mIf True, we make Othello boards instead of sequences (requires OthelloGPT)\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mn_boards_per_row: 3\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mOnly relevant for Othello, sets number of boards per row in top examples\u001b[0m\n",
              "│       ├── \u001b[1;38;2;0;170;0mdfa_for_attn_saes: True\u001b[0m \n",
              "│       │   \u001b[3;38;2;136;136;136mOnly relevant for attention SAEs. If true, shows DFA for top attention tokens.\u001b[0m\n",
              "│       └── \u001b[1;38;2;0;170;0mdfa_buffer: (5, 5)\u001b[0m \n",
              "│           \n",
              "│           \n",
              "└── Column 1\n",
              "    ├── ActsHistogram\n",
              "    │   └── \u001b[1;38;2;0;170;0mn_bins: 50\u001b[0m \n",
              "    │       \u001b[3;38;2;136;136;136mNumber of bins for the histogram.\u001b[0m\n",
              "    │       \n",
              "    └── FeatureTables\n",
              "        ├── \u001b[1;38;5;208mn_rows: 5\u001b[0m (default = 3) \n",
              "        │   \u001b[3;38;2;136;136;136mNumber of rows to show for each feature table.\u001b[0m\n",
              "        ├── \u001b[1;38;2;0;170;0mneuron_alignment_table: True\u001b[0m \n",
              "        │   \u001b[3;38;2;136;136;136mWhether to show the neuron alignment table.\u001b[0m\n",
              "        ├── \u001b[1;38;2;0;170;0mcorrelated_neurons_table: True\u001b[0m \n",
              "        │   \u001b[3;38;2;136;136;136mWhether to show the correlated neurons table.\u001b[0m\n",
              "        ├── \u001b[1;38;2;0;170;0mcorrelated_features_table: True\u001b[0m \n",
              "        │   \u001b[3;38;2;136;136;136mWhether to show the (pairwise) correlated features table.\u001b[0m\n",
              "        └── \u001b[1;38;2;0;170;0mcorrelated_b_features_table: False\u001b[0m \n",
              "            \u001b[3;38;2;136;136;136mWhether to show the correlated encoder-B features table.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "addd43e6648c4dc7a40b43bdb3f3cbca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Forward passes to cache data for vis:   0%|          | 0/64 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "704c2f774be74e0084718e8a4ae0d747",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting vis data from cached data:   0%|          | 0/256 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time   </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│ (1) Forward passes to gather model activations │ 3.95s  │ 13.1% │\n",
              "│ (2) Getting data for sequences                 │ 23.00s │ 76.6% │\n",
              "│ (3) Getting data for non-sequence components   │ 1.32s  │ 4.4%  │\n",
              "│ (?) Unaccounted time                           │ 1.78s  │ 5.9%  │\n",
              "└────────────────────────────────────────────────┴────────┴───────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│ (1) Forward passes to gather model activations │ 3.95s  │ 13.1% │\n",
              "│ (2) Getting data for sequences                 │ 23.00s │ 76.6% │\n",
              "│ (3) Getting data for non-sequence components   │ 1.32s  │ 4.4%  │\n",
              "│ (?) Unaccounted time                           │ 1.78s  │ 5.9%  │\n",
              "└────────────────────────────────────────────────┴────────┴───────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"850\"\n",
              "            src=\"demo_feature_vis_custom.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f59556e8cd0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# [2/5] Custom layout demo\n",
        "\n",
        "from sae_vis.data_config_classes import (\n",
        "    ActsHistogramConfig,\n",
        "    Column,\n",
        "    FeatureTablesConfig,\n",
        "    SeqMultiGroupConfig,\n",
        ")\n",
        "\n",
        "layout = SaeVisLayoutConfig(\n",
        "    columns=[\n",
        "        Column(\n",
        "            SeqMultiGroupConfig(buffer=None, n_quantiles=0, top_acts_group_size=30),\n",
        "            width=1000,\n",
        "        ),\n",
        "        Column(ActsHistogramConfig(), FeatureTablesConfig(n_rows=5), width=500),\n",
        "    ],\n",
        "    height=1000,\n",
        ")\n",
        "layout.help()\n",
        "\n",
        "sae_vis_data_custom = SaeVisData.create(\n",
        "    sae=sae,\n",
        "    sae_B=sae_B,\n",
        "    model=model,\n",
        "    tokens=all_tokens[:4096, :48],  # 4096\n",
        "    cfg=SaeVisConfig(\n",
        "        features=range(256),  # 256\n",
        "        feature_centric_layout=layout,\n",
        "    ),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "filename = \"demo_feature_vis_custom.html\"\n",
        "sae_vis_data_custom.save_feature_centric_vis(filename, feature=8)\n",
        "display_vis_inline(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fc1NNWcYyQMC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"850\"\n",
              "            src=\"demo_prompt_vis.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f597c1f0ed0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# [3/5] Prompt-centric vis\n",
        "\n",
        "prompt = \"'first_name': ('django.db.models.fields\"\n",
        "seq_pos = model.tokenizer.tokenize(prompt).index(\"Ġ('\")\n",
        "metric = \"act_quantile\"\n",
        "\n",
        "filename = \"demo_prompt_vis.html\"\n",
        "sae_vis_data.save_prompt_centric_vis(filename, prompt=prompt, seq_pos=seq_pos, metric=metric)\n",
        "display_vis_inline(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrzWdLRLybOK"
      },
      "outputs": [],
      "source": [
        "# [4/5] OthelloGPT\n",
        "\n",
        "# This one is a bit more complicated because I've included linear probes in the vis! They tell you\n",
        "# the extent to which any given SAE latent reads from / writes to some particular probe direction.\n",
        "\n",
        "sae_vis_data = SaeVisData.create(\n",
        "    sae=othellogpt_sae,\n",
        "    model=othellogpt,  # type: ignore\n",
        "    linear_probes=[\n",
        "        (\"input\", \"theirs vs mine\", othello_linear_probes[\"theirs vs mine\"]),\n",
        "        (\"output\", \"theirs vs mine\", othello_linear_probes[\"theirs vs mine\"]),\n",
        "        (\"input\", \"empty\", othello_linear_probes[\"empty\"]),\n",
        "        (\"output\", \"empty\", othello_linear_probes[\"empty\"]),\n",
        "    ],\n",
        "    tokens=othello_tokens,\n",
        "    target_logits=othello_target_logits,\n",
        "    cfg=SaeVisConfig(\n",
        "        features=othello_alive_feats[:16],\n",
        "        seqpos_slice=(5, -5),\n",
        "        feature_centric_layout=SaeVisLayoutConfig.default_othello_layout(),\n",
        "    ),\n",
        "    vocab_dict=load_othello_vocab(),\n",
        "    verbose=True,\n",
        "    clear_memory_between_batches=True,\n",
        ")\n",
        "\n",
        "filename = \"demo_othello_vis.html\"\n",
        "sae_vis_data.save_feature_centric_vis(filename, verbose=True)\n",
        "display_vis_inline(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIgtAdTnziY8"
      },
      "outputs": [],
      "source": [
        "# [5/5] Attention SAE\n",
        "\n",
        "sae_vis_data = SaeVisData.create(\n",
        "    sae=attn_sae,\n",
        "    model=attn_model,\n",
        "    tokens=tokens,\n",
        "    cfg=SaeVisConfig(features=attn_alive_feats[:32]),\n",
        "    verbose=True,\n",
        "    clear_memory_between_batches=True,\n",
        ")\n",
        "\n",
        "filename = \"demo_feature_vis_attn2l.html\"\n",
        "sae_vis_data.save_feature_centric_vis(filename)\n",
        "display_vis_inline(filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KYWV3AuAxXjO"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
