# @package _global_

# Model configuration for TinyStories
model_name: "pythia-70m"
d_in: 512

# Standard SAE architecture
architecture: "topk"
activation_fn: null
activation_fn_kwargs:
  k: 32
  postact_fn: "relu"
expansion_factor: 64

hook_name: "blocks.3.hook_mlp_out"
hook_layer: 3

# Dataset
# dataset_path: "togethercomputer/RedPajama-Data-1T-Sample"
# is_dataset_tokenized: false
dataset_path: "sidnb13/rp1t-sample-tokenized-pythia-70m"
is_dataset_tokenized: true
column_name: text

use_cached_activations: true
cached_activations_path: "sidnb13/rp1t-sample-tokenized-acts-pythia-70m-blocks.3.hook_mlp_out"

# Training settings
lr: 1e-4
training_tokens: 1_000_000
train_batch_size_tokens: 16384
store_batch_size_prompts: 64
n_batches_in_buffer: 256

# Wandb project
wandb_project: "pythia-70m-redpajama" 
wandb_group: "topk-sae-debug"
log_to_wandb: false
