# @package _global_

# Model configuration for TinyStories
model_name: "pythia-70m"
d_in: 512

# Standard SAE architecture
architecture: "topk"
activation_fn: null
activation_fn_kwargs:
  k: 32
  postact_fn: "relu"
expansion_factor: 64
use_fast_kernels: true

# Dataset
dataset_path: "togethercomputer/RedPajama-Data-1T-Sample"
is_dataset_tokenized: false
column_name: text
streaming: false

# Training settings
lr_warm_up_steps: 500
training_tokens: 1_000_000_000
feature_sampling_window: 12500
dead_feature_window: 6250

flop_profile_interval: 1000

train_batch_size_tokens: 65536
store_batch_size_prompts: 64
n_batches_in_buffer: 256
lr: 1e-4
lr_scheduler_name: "constant"

# Wandb project
wandb_project: "pythia-70m-redpajama" 
