# @package _global_

# Model configuration for TinyStories
model_name: "pythia-70m"
d_in: 512

# Standard SAE architecture
architecture: "topk"
activation_fn: null
activation_fn_kwargs:
  k: 32
  postact_fn: "relu"
expansion_factor: 64
normalize_activations: "layer_norm"

# Dataset
dataset_path: "sidnb13/rp1t-sample-tokenized-pythia-70m"
is_dataset_tokenized: true
column_name: text

# dataset_path: "monology/pile-uncopyrighted"
# is_dataset_tokenized: false
# column_name: text
# streaming: true

hook_name: blocks.3.hook_mlp_out 
hook_layer: 3

# Training settings
lr_warm_up_steps: 0
training_tokens: 500_000_000
feature_sampling_window: 100 
dead_feature_window: 50
dead_feature_threshold: 1e-3

enable_auxk_loss: true
enable_dead_neuron_bias_boosting: false

flop_profile_interval: 1000

train_batch_size_tokens: 65536
n_batches_in_buffer: 256

lr: 1e-4
lr_scheduler_name: "constant"
dtype: "bfloat16"

# Wandb project
wandb_project: "pythia-70m-redpajama" 
wandb_group: "topk-sae-medium"

checkpoint_path: "checkpoints/topk-sae-medium"
