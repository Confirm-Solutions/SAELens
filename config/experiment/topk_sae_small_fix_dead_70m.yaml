# @package _global_

defaults:
  - override /hydra/sweeper: list

# Model configuration for TinyStories
model_name: "pythia-70m"
d_in: 512

# Standard SAE architecture
architecture: "topk"
activation_fn: null
activation_fn_kwargs:
  k: 32
  postact_fn: "relu"
expansion_factor: 64

hook_name: "blocks.2.hook_mlp_out"
hook_layer: 2

# Dataset
dataset_path: "togethercomputer/RedPajama-Data-1T-Sample"
is_dataset_tokenized: false
column_name: text

# Training settings
lr: 1e-4
lr_warm_up_steps: 2000
training_tokens: 100_000_000
train_batch_size_tokens: 16384
store_batch_size_prompts: 64
n_batches_in_buffer: 256
dead_feature_window: 500

hydra:
  sweeper:
    list_params:
      enable_auxk_loss: [true, false, false, false, false]
      enable_dead_neuron_bias_boosting: [false, true, true, true, true]
      dead_neuron_bias_boost_scale: [null, 1e-5, 1e-4, 1e-3, 1e-2]

# Wandb project
wandb_project: "pythia-70m-redpajama"
wandb_group: "topk-sae-sweep-debug-dead"
