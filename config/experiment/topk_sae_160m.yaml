# @package _global_

defaults:
  - override /hydra/sweeper: list

hydra:
  sweeper:
    grid_params:
      activation_fn_kwargs.k: 8,16,32
    list_params:
      expansion_factor:
        - 64
        - 128
      train_batch_size_tokens:
        - 65536
        - 32768

model_name: "pythia-160m-deduped"
d_in: 768

architecture: "topk"
activation_fn: null
activation_fn_kwargs:
  k: 32
  postact_fn: "relu"
normalize_activations: "layer_norm"

dataset_path: "togethercomputer/RedPajama-Data-1T-Sample"
is_dataset_tokenized: false
column_name: text

hook_name: blocks.4.hook_resid_post 
hook_layer: 4

# Training settings
lr_warm_up_steps: 500
training_tokens: 1_000_000_000
feature_sampling_window: 1000
dead_feature_window: 75
dead_feature_threshold: 1e-3

enable_auxk_loss: true
enable_dead_neuron_bias_boosting: false

flop_profile_interval: 1000

train_batch_size_tokens: 65536
n_batches_in_buffer: 256
store_batch_size_prompts: 32

lr: 3e-4
lr_scheduler_name: "constant"
dtype: "bfloat16"

# Wandb project
wandb_project: "pythia-160m-deduped-rp1t" 
wandb_group: "topk-sae-resid-post-160m-1b-sweep"

checkpoint_path: "checkpoints/topk-sae-resid-post-160m-1b-sweep"
