# @package _global_

# Model configuration for TinyStories
model_name: "pythia-160m-deduped"
d_in: 768
tokenizer_name: "EleutherAI/pythia-160m-deduped"

# Ridge architecture
architecture: "ridge"
activation_fn: null
activation_fn_kwargs:
  k: 32
  alpha: 2
expansion_factor: 64

# Dataset
dataset_path: "togethercomputer/RedPajama-Data-1T-Sample"
is_dataset_tokenized: true
column_name: text

hook_name: blocks.4.hook_resid_post 
hook_layer: 4

# Training settings
lr_warm_up_steps: 500
training_tokens: 1_000_000_000
feature_sampling_window: 12500
dead_feature_window: 6250
train_batch_size_tokens: 4096
store_batch_size_prompts: 32
n_batches_in_buffer: 64
lr: 3e-4
lr_scheduler_name: "constant"

# Wandb project
wandb_project: "pythia-160m-deduped-rp1t" 
wandb_group: "ridge-resid-post-160m-1b-sweep"
checkpoint_path: "checkpoints/ridge-resid-post-160m-1b-sweep"
